Columns:
- Name - same names for more loans(Subway) - usaually bigger companies
		- add feature(Boolean) for bigger company (has more loans than n) and has good PIF/CHROFF rate, 
		good_rate_comps are just 5000 samples DONT DO THIS
	- 10 nans 
	- 83522 names end with INC. rate: P I F     0.882378   CHGOFF    0.117622
		                          P I F     0.816746  CHGOFF    0.183254(doesnt end)
	      Endings with bad rates Inc, Inc., LLC
	- PREPROCESSING: Change name variable to Endings 
		-done using get_endings() 
			
- City - 23 nans 
	- remove Parenthesis and stuff in them in city names 
	- 29799 unique city names 
 	- PREPROCESSING: droping col due to high amount of nuique city names 
		- done using drop_col(df)

- State - 51 states 
	
	- PREPROCESSING: make mutliple types of categories to replace all 51 states bcs its too much for onehot,
			group them based on similar chrgoff_rates
		- done using states_to_rate_categories(df, label), 
		-try more categories 
	
- Zip - too much nunique values and we already got state column to handle location 
	- PREPROCESSING: droping col due to high amount of nuique city names 
		- done using drop_city_col(df)

- Bank - PREPROCESSING: droping col due to high amount of nuique names and bank name shouldnt be important

- BankState - banks in the same state as bussiness are having bettr chrgoff_rates
	- PREPROCESSING: 
			- drop null (70)
			- calssifing PR GU AN EN VI as Other due to low amount of samples
			- done using replace_small_states
			- modified this col which is true if state and bank state are same 

- NAICS - a lot of 0s which are null, i will leave them as a value that naics code wasnt specified 
	- PREPROCESSING: 
		- taken just first two numbers(sector) of the whole code
		- value Other was asigned to the 6 smallest sectors 

- ApprovalDate - 
		- PREPROCESSING:
			- convert to datetime objects 

- ApprovalFY - 
		- PREPROCESSING:
			- need to clean it to all being ints bcs there are strings and one value has A th the end

- Term - longer terms tend to have better chrgoff rates, a lot of less represented terms 
	- most used Terms[12, 36, 48, 60, 84, 120, 180, 240, 300] 
		- PREPROCESSING: remove 0s, value with less than x samples merge with next one/categorize them

- NoEmp - 5000 0s(explore), leave them there 
	- PREPROCESSING: - combine low frequency values in the upper range 

- NewExist - might drop NewEXist seems to have no influence on chrgoff_rate
	- NO PREPROCESSING: droping col bcs of no influence on chrgoff rate 

- CreateJob - 
	- PREPROCESSING: group_values that have similar value counts and chrgoff rates

- RetainJob -  
	- PREPROCESSING: group_values that have similar value counts and chrgoff rates

- FranchiseCode - 
	- PREPROCESSING: grouped all values above 1 to category named 2 

- UrbanRural - 
	-NO PREPROCESSING:

- RevLineCr - has some wierd values like T, ., -
	- PREPROCESSING: rename 0s as N due to same chrgoff_rate, drop the wierd values, 

- LowDoc - has some wierd less frequent values 
	- PREPROCESSING: drop those wierd values 

- ChgOffDate - 
	- PREPROCESSING: droping column bcs of no usefull info

- DisbursementDate - 
	- PREPROCESSING:
			- convert to datetime objects

- DisbursementGross -  
	- PREPROCESSING: 
		- droping col bcs of high corr with sba_appv

- GrAppv -  
	- PREPROCESSING: 
		- droping col bcs of high corr with sba_app 

- SBA_Appv-  
	- PREPROCESSING:
		- cleaning the string and converting it to int64


- BalanceGross - doesnt undersatnd it, almost all values are the same
	- PREPROCESSING: droping col for no usefull indormation 
	
- MIS_Status(label) - 
	- PREPROCESSING: Replaced with true false values, renamed col to Default 

- ChgOffPrinGr - 
	- PREPROCESSING: Droping the column bcs of no usefull info 
	
Todos:

  -Todos during modeling and testing:
	- used architectures: 90, 45/    20, 20/    80, Dropout, 80, Dropout/    20,15,4/


General preprocessing:
Nulls dropped - ['Name', 'City', 'State', 'Bank', 'BankState', 'NewExist', 'RevLineCr', 'LowDoc', 
		'DisbursementDate']


pd.set_option('display.max_rows', 10)
